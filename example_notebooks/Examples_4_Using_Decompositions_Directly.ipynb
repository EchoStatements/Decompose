{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Using Decomposition Objects\n",
    "\n",
    "So far, the examples have covered how to use `BVDExperiment` to train models and calculate statistics (bias, variance, etc) in a single function.\n",
    "\n",
    "However, the decomposition objects can be used in a standalone way, using predictions gathered from models trained and evaluated externally to this library.\n",
    "\n",
    "## Cross Entropy & Squared Loss Decompositions\n",
    "\n",
    "In the first example, we show how the `CrossEntropy` object can be used to get bias, variance and diversity for an ensemble using randomly generated data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias-Variance Decomposition\n",
      "---------------------------\n",
      "expected risk: 1.1313417337555043\n",
      "\t=\n",
      "ensemble bias: 1.099645892902822\n",
      "\t+\n",
      "ensemble variance: 0.03169584085268233\n",
      "\n",
      "Bias-Variance-Diversity Decomposition\n",
      "-------------------------------------\n",
      "expected risk: 1.1313417337555043\n",
      "\t=\n",
      "average bias: 1.1026982117921316\n",
      "\t+\n",
      "average variance: 0.23516614892180485\n",
      "\t-\n",
      "diversity: 0.20652262695843182\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "ensemble_size = 10\n",
    "n_examples = 500\n",
    "n_classes = 3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Labels are integers corresponding to the correct class\n",
    "labels = np.random.choice(n_classes, size=(n_examples,))\n",
    "# Predictions are in the form of one hot vectors\n",
    "preds = np.random.uniform(0, 1 ,(n_trials, ensemble_size, n_examples, n_classes))\n",
    "preds = preds / preds.sum(axis=3, keepdims=True)\n",
    "\n",
    "from decompose import CrossEntropy\n",
    "\n",
    "decomposition = CrossEntropy(preds, labels)\n",
    "\n",
    "print(\"Bias-Variance Decomposition\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"ensemble bias: {decomposition.ensemble_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"ensemble variance: {decomposition.ensemble_variance.mean()}\")\n",
    "\n",
    "print(\"\\nBias-Variance-Diversity Decomposition\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"average bias: {decomposition.average_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"average variance: {decomposition.average_variance.mean()}\")\n",
    "print(\"\\t-\")\n",
    "print(f\"diversity: {decomposition.diversity.mean()}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the squared loss in the same way, with the only difference being the shape of the predictions array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias-Variance Decomposition\n",
      "---------------------------\n",
      "expected risk: 1.4113203905210603\n",
      "\t=\n",
      "ensemble bias: 1.3112493974383124\n",
      "\t+\n",
      "ensemble variance: 0.10007099308274785\n",
      "\n",
      "Bias-Variance-Diversity Decomposition\n",
      "-------------------------------------\n",
      "expected risk: 1.4113203905210603\n",
      "\t=\n",
      "average bias: 1.3206010421925793\n",
      "\t+\n",
      "average variance: 0.9922307885441782\n",
      "\t-\n",
      "diversity: 0.9015114402156975\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "ensemble_size = 10\n",
    "n_examples = 500\n",
    "n_classes = 3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# real-valued labels\n",
    "labels = np.random.normal(0.5, 1., size=(n_examples,))\n",
    "# real-values predictions\n",
    "preds = np.random.normal(0, 1 ,(n_trials, ensemble_size, n_examples))\n",
    "\n",
    "from decompose import SquaredLoss\n",
    "\n",
    "decomposition = SquaredLoss(preds, labels)\n",
    "print(\"Bias-Variance Decomposition\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"ensemble bias: {decomposition.ensemble_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"ensemble variance: {decomposition.ensemble_variance.mean()}\")\n",
    "\n",
    "print(\"\\nBias-Variance-Diversity Decomposition\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"average bias: {decomposition.average_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"average variance: {decomposition.average_variance.mean()}\")\n",
    "print(\"\\t-\")\n",
    "print(f\"diversity: {decomposition.diversity.mean()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0-1 Loss Effect Decomposition\n",
    "\n",
    "The effect decomposition for the 0-1 loss works in the same way but takes class label predictions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias-variance decomposition\n",
      "---------------------------\n",
      "expected risk: 0.27218000000000003\n",
      "\t=\n",
      "ensemble bias: 0.0\n",
      "\t+\n",
      "ensemble variance: 0.27218000000000003\n",
      "\n",
      "bias-variance-diversity effect decomposition\n",
      "--------------------------------------------\n",
      "expected risk: 0.27218000000000003\n",
      "\t=\n",
      "average bias: 0.0032\n",
      "\t+\n",
      "average variance: 0.496404\n",
      "\t-\n",
      "diversity: 0.227424\n"
     ]
    }
   ],
   "source": [
    "n_trials = 100\n",
    "ensemble_size = 10\n",
    "n_examples = 500\n",
    "n_classes = 3\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Labels are integers corresponding to the correct class\n",
    "labels = np.random.choice(n_classes, size=(n_examples,))\n",
    "# Predictions are in the form of class labels.\n",
    "# We randomly generate predictions so that half of all member predictions are correct (in an i.i.d. way)\n",
    "preds = np.broadcast_to(labels, shape=(n_trials, ensemble_size, n_examples))\n",
    "incorrect_preds = np.random.choice([0, 1], size=(n_trials, ensemble_size, n_examples), p=[0.5, 0.5])\n",
    "preds = (preds + incorrect_preds * np.random.choice([1, 2], size=(n_trials, ensemble_size, n_examples))) % n_classes\n",
    "\n",
    "from decompose import ZeroOneLoss\n",
    "\n",
    "decomposition = ZeroOneLoss(preds, labels)\n",
    "\n",
    "\n",
    "print(f\"bias-variance decomposition\")\n",
    "print(f\"---------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"ensemble bias: {decomposition.ensemble_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"ensemble variance: {decomposition.ensemble_variance_effect.mean()}\")\n",
    "\n",
    "print(f\"\\nbias-variance-diversity effect decomposition\")\n",
    "print(f\"--------------------------------------------\")\n",
    "print(f\"expected risk: {decomposition.expected_ensemble_loss.mean()}\")\n",
    "print(\"\\t=\")\n",
    "print(f\"average bias: {decomposition.average_bias.mean()}\")\n",
    "print(\"\\t+\")\n",
    "print(f\"average variance: {decomposition.average_variance_effect.mean()}\")\n",
    "print(\"\\t-\")\n",
    "print(f\"diversity: {decomposition.diversity_effect.mean()}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Experiment\n",
    "\n",
    "In this example, we show how a simple experiment script may look. We train a collection of ensembles, each on 90% of the total available training data. Collecting the predictions from each ensemble member in each trial, we are then able to create an instance of the `CrossEntropy` decomposition class from which we get bias, variance and diversity estimates.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06893953762076134\n"
     ]
    }
   ],
   "source": [
    "from decompose.data_utils import load_standard_dataset\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from decompose import CrossEntropy\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_standard_dataset(\"digits\", frac_training=0.5, normalize_data=True)\n",
    "\n",
    "\n",
    "\n",
    "n_trials = 10\n",
    "ensemble_size = 5\n",
    "\n",
    "# We construct a numpy array with the predictions of ensemble members over numerous trials,\n",
    "# the shape of the numpy array is (n_trials, ensemble_size, test_data_size, n_classes)\n",
    "# for regression, the n_classes dimension is omitted.\n",
    "model_outputs = np.zeros((n_trials, ensemble_size, test_data.shape[0], 10))\n",
    "\n",
    "\n",
    "for t_idx in range(n_trials):\n",
    "    subsample_size = int(train_data.shape[0] * 0.9)\n",
    "    subsample_indices = np.random.permutation(train_data.shape[0])[:subsample_size]\n",
    "    train_data_sample = train_data[subsample_indices, :]\n",
    "    train_labels_sample = train_labels[subsample_indices]\n",
    "    # ignore covergence warnings from MLPClassifier\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    ensemble = BaggingClassifier(n_estimators=ensemble_size,\n",
    "                                 base_estimator=MLPClassifier(hidden_layer_sizes=20,\n",
    "                                                              max_iter=100,\n",
    "                                                              learning_rate_init=0.001,\n",
    "                                                              alpha=0.1,\n",
    "                                                              momentum=0,\n",
    "                                                              solver=\"adam\"))\n",
    "    ensemble.fit(train_data_sample, train_labels_sample)\n",
    "    for e_idx, estimator in enumerate(ensemble.estimators_):\n",
    "        model_outputs[t_idx, e_idx, :, :] = estimator.predict_proba(test_data)\n",
    "\n",
    "bvd_decomposition = CrossEntropy(model_outputs, test_labels)\n",
    "\n",
    "\n",
    "# Add noise to deal with fact geometric mean isn't defined for zero\n",
    "model_outputs = (1 - 1e-9) * model_outputs + 1e-10 * np.ones_like(model_outputs)\n",
    "\n",
    "# We can now print out quantities from the decomposition for instance the diversity is\n",
    "print(bvd_decomposition.diversity.mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "decomposition_env",
   "language": "python",
   "display_name": "decomposition_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}